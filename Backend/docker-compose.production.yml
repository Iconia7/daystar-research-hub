version: '3.8'

# ============================================================================
# DAYSTAR RESEARCH COLLABORATION GRAPH - PRODUCTION-READY DOCKER SETUP
# ============================================================================
# 
# Improvements over basic setup:
# ✅ pgvector/pgvector:pg15 - Pre-installed pgvector extension
# ✅ Redis persistence with AOF
# ✅ Resource limits on all services
# ✅ Proper health checks with dependencies
# ✅ Secrets from .env file (not hardcoded)
# ✅ Database backup service
# ✅ Static files volume for production
# ✅ Nginx reverse proxy with proper configuration
# ✅ Initialization script with migrations and seeding
#
# Usage:
#   docker-compose up --build          Build and start all services
#   docker-compose logs -f web         Follow web app logs
#   docker-compose exec web bash       Execute commands in web container
#   docker-compose down                Stop all services
#   docker-compose down -v             Stop and remove volumes (WARNING: data loss!)
#
# ============================================================================

services:
  # ========== PostgreSQL Database with pgvector ==========
  postgres:
    # Using pgvector pre-built image instead of plain postgres
    # This ensures pgvector extension is available
    image: pgvector/pgvector:pg15
    container_name: daystar-postgres
    
    # Load secrets from .env file
    env_file: .env
    
    environment:
      # Database name (also in .env)
      POSTGRES_DB: ${DB_NAME:-daystar_db}
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-postgres}
      
      # pgvector initialization
      POSTGRES_INITDB_ARGS: "-c shared_preload_libraries=vector"
    
    volumes:
      # Persistent data storage
      - postgres_data:/var/lib/postgresql/data
      
      # Database initialization scripts
      # Numbered prefixes ensure execution order
      - ./docker-init-db.sql:/docker-entrypoint-initdb.d/01-init.sql
      - ./scripts/init-seeding.sql:/docker-entrypoint-initdb.d/02-seed.sql
    
    ports:
      - "${DB_PORT:-5432}:5432"
    
    # Health check - waits for database to be ready
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    
    # Resource limits to prevent runaway memory/cpu usage
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    
    networks:
      - daystar-network
    
    restart: unless-stopped

  # ========== Redis for Celery Task Queue ==========
  redis:
    image: redis:7-alpine
    container_name: daystar-redis
    
    # Enable AOF (Append-Only File) for data persistence
    # Tasks won't be lost on restart
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis}
    
    ports:
      - "6379:6379"
    
    volumes:
      # Persistent data with AOF
      - redis_data:/data
    
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-redis}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    
    networks:
      - daystar-network
    
    restart: unless-stopped

  # ========== Django Web Application ==========
  web:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: daystar-web
    
    # Load environment variables from .env
    env_file: .env
    
    environment:
      # Database connection
      DATABASE_URL: postgresql://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:${DB_PORT:-5432}/${DB_NAME:-daystar_db}
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis}@redis:6379/0
      
      # Django settings
      DEBUG: ${DEBUG:-False}
      SECRET_KEY: ${SECRET_KEY:-change-me-in-production}
      ALLOWED_HOSTS: ${ALLOWED_HOSTS:-localhost,127.0.0.1,web}
      CORS_ALLOWED_ORIGINS: ${CORS_ALLOWED_ORIGINS:-http://localhost:3000,http://localhost:8000}
    
    # Startup command: migrate → collect static → run gunicorn
    command: >
      sh -c "
        echo 'Running Django migrations...' &&
        python manage.py migrate --noinput &&
        echo 'Collecting static files...' &&
        python manage.py collectstatic --noinput &&
        echo 'Starting Gunicorn...' &&
        gunicorn daystar_project.wsgi 
          --bind 0.0.0.0:8000 
          --workers 4 
          --worker-class sync 
          --timeout 120 
          --access-logfile - 
          --error-logfile - 
          --log-level info
      "
    
    ports:
      - "8000:8000"
    
    # Wait for database and redis to be healthy
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    
    volumes:
      # Mount source code for development
      - ./:/app
      
      # Shared static files volume
      - static_files:/app/staticfiles
    
    # Health check for the web service
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    
    networks:
      - daystar-network
    
    restart: unless-stopped

  # ========== Celery Worker for Background Tasks ==========
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: daystar-celery-worker
    
    env_file: .env
    
    environment:
      DATABASE_URL: postgresql://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:${DB_PORT:-5432}/${DB_NAME:-daystar_db}
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis}@redis:6379/0
      DEBUG: ${DEBUG:-False}
      SECRET_KEY: ${SECRET_KEY:-change-me-in-production}
    
    # Run Celery worker
    command: celery -A daystar_project worker -l info --concurrency=4 --prefetch-multiplier=1
    
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    
    volumes:
      - ./:/app
    
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    
    networks:
      - daystar-network
    
    restart: unless-stopped

  # ========== Celery Beat for Scheduled Tasks ==========
  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: daystar-celery-beat
    
    env_file: .env
    
    environment:
      DATABASE_URL: postgresql://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:${DB_PORT:-5432}/${DB_NAME:-daystar_db}
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis}@redis:6379/0
      DEBUG: ${DEBUG:-False}
      SECRET_KEY: ${SECRET_KEY:-change-me-in-production}
    
    # Run Celery Beat scheduler
    command: celery -A daystar_project beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    
    volumes:
      - ./:/app
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    
    networks:
      - daystar-network
    
    restart: unless-stopped

  # ========== Database Backup Service ==========
  # Automatically backs up the database daily to ./backups/
  pg-backup:
    image: postgres:15-alpine
    container_name: daystar-pg-backup
    
    # Backup script: runs every 24 hours, keeps last 30 days
    command: >
      sh -c "
        while true; do
          echo 'Creating database backup...' &&
          pg_dump -h postgres -U ${DB_USER:-postgres} ${DB_NAME:-daystar_db} | 
          gzip > /backups/daystar_backup_$$(date +\%Y\%m\%d_\%H\%M\%S).sql.gz &&
          echo 'Backup completed at '$$(date) &&
          find /backups -name '*.sql.gz' -mtime +30 -delete &&
          echo 'Cleaned up backups older than 30 days' &&
          sleep 86400;
        done
      "
    
    environment:
      PGPASSWORD: ${DB_PASSWORD:-postgres}
    
    volumes:
      # Backup destination
      - ./backups:/backups
    
    depends_on:
      postgres:
        condition: service_healthy
    
    networks:
      - daystar-network
    
    restart: unless-stopped

  # ========== Nginx Reverse Proxy ==========
  # Production-like reverse proxy for handling requests
  nginx:
    image: nginx:alpine
    container_name: daystar-nginx
    
    ports:
      - "80:80"
      - "443:443"
    
    volumes:
      # Nginx configuration
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      
      # SSL certificates (for production)
      - ./ssl:/etc/nginx/ssl:ro
      
      # Shared static files
      - static_files:/app/staticfiles:ro
    
    depends_on:
      - web
    
    networks:
      - daystar-network
    
    restart: unless-stopped

# ============================================================================
# VOLUMES - Persistent Data Storage
# ============================================================================

volumes:
  # PostgreSQL data - survives container restarts
  postgres_data:
    driver: local
  
  # Redis data with AOF persistence
  redis_data:
    driver: local
  
  # Static files served by Nginx
  static_files:
    driver: local

# ============================================================================
# NETWORKS - Inter-service Communication
# ============================================================================

networks:
  daystar-network:
    driver: bridge
